{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc909aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import scipy.stats as stats\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbd36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets grab our data from the pca transformation\n",
    "X_numpy = np.load('pca_transformed_data_v1.npy', allow_pickle=True)\n",
    "y_numpy = np.load('label.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d99f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_numpy.astype(np.float32)).squeeze()\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1fe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets split our data\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2,\n",
    "                                                                           random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64252ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the data into gpu\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "X_train,X_test, y_train,y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06663ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a model for hyperparameter tuning\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(18, 34)\n",
    "        self.layer2 = nn.Linear(34, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #add forward function\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.relu(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0895ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1605694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function & optimizer\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0009289305355089368, weight_decay=0.0007777563951641034)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49aba8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamincheng/miniforge3/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | train_loss: 0.7189545035362244 | test_loss: 0.6860600709915161 | test_spearman: 0.002595686186852043\n",
      "Epoch: 465 | train_loss: 0.579241931438446 | test_loss: 0.5426278710365295 | test_spearman: 0.0999170834943905\n",
      "Epoch: 930 | train_loss: 0.5695499777793884 | test_loss: 0.5350172519683838 | test_spearman: 0.14629353790045133\n",
      "Epoch: 1395 | train_loss: 0.5646200776100159 | test_loss: 0.5303875207901001 | test_spearman: 0.1678125485410012\n",
      "Epoch: 1860 | train_loss: 0.5612270832061768 | test_loss: 0.527455747127533 | test_spearman: 0.18814033585107512\n",
      "Epoch: 2325 | train_loss: 0.5584163069725037 | test_loss: 0.5253605246543884 | test_spearman: 0.19837313954237665\n",
      "Epoch: 2790 | train_loss: 0.556374728679657 | test_loss: 0.523381769657135 | test_spearman: 0.21088019889306808\n",
      "Epoch: 3255 | train_loss: 0.5548725724220276 | test_loss: 0.5219380259513855 | test_spearman: 0.22477937565389927\n",
      "Epoch: 3720 | train_loss: 0.5536481142044067 | test_loss: 0.5208061337471008 | test_spearman: 0.23569140335085756\n",
      "Epoch: 4185 | train_loss: 0.5525901317596436 | test_loss: 0.5199167728424072 | test_spearman: 0.24314221684846796\n",
      "Epoch: 4650 | train_loss: 0.5517174005508423 | test_loss: 0.519376277923584 | test_spearman: 0.24906084823682104\n",
      "Epoch: 5115 | train_loss: 0.5509806871414185 | test_loss: 0.51896071434021 | test_spearman: 0.2557610586032628\n",
      "Epoch: 5580 | train_loss: 0.5503448247909546 | test_loss: 0.5185852646827698 | test_spearman: 0.25931542480603026\n",
      "Epoch: 6045 | train_loss: 0.5497915744781494 | test_loss: 0.5181836485862732 | test_spearman: 0.2628482425935488\n",
      "Epoch: 6510 | train_loss: 0.5492974519729614 | test_loss: 0.5178441405296326 | test_spearman: 0.26767374083333995\n",
      "Epoch: 6975 | train_loss: 0.5488566160202026 | test_loss: 0.5175715088844299 | test_spearman: 0.270600958367302\n",
      "Epoch: 7440 | train_loss: 0.548483669757843 | test_loss: 0.5173465609550476 | test_spearman: 0.27403658382354185\n",
      "Epoch: 7905 | train_loss: 0.5481284856796265 | test_loss: 0.5171705484390259 | test_spearman: 0.27734404110158273\n",
      "Epoch: 8370 | train_loss: 0.5478019118309021 | test_loss: 0.5170139074325562 | test_spearman: 0.27802034709267454\n",
      "Epoch: 8835 | train_loss: 0.547501266002655 | test_loss: 0.5168835520744324 | test_spearman: 0.278563322264414\n",
      "Epoch: 9300 | train_loss: 0.5472158193588257 | test_loss: 0.5167481303215027 | test_spearman: 0.28023534461013005\n"
     ]
    }
   ],
   "source": [
    "#train and test loop\n",
    "torch.mps.manual_seed(23)\n",
    "torch.manual_seed(4)\n",
    "\n",
    "epochs = 9308\n",
    "subset = int(epochs * 0.05)\n",
    "epoch_count = []\n",
    "training_loss = []\n",
    "testing_loss = []\n",
    "for epoch in range(epochs):\n",
    "    #turn on training mode\n",
    "    model.train()\n",
    "    \n",
    "    #fit data\n",
    "    train_pred = model(X_train)\n",
    "    \n",
    "    #calculate loss\n",
    "    train_loss = loss_fn(train_pred, y_train)\n",
    "    \n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #back propagation\n",
    "    train_loss.backward()\n",
    "    \n",
    "    #gradient descent\n",
    "    optimizer.step()\n",
    "    \n",
    "    #testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "    \n",
    "    if epoch % subset == 0:\n",
    "        training_loss.append(train_loss)\n",
    "        epoch_count.append(epoch)\n",
    "        testing_loss.append(test_loss)\n",
    "        pred = test_pred.to('cpu')\n",
    "        accuracy = stats.spearmanr(y_test.to('cpu'),pred)\n",
    "        print(f'Epoch: {epoch} | train_loss: {train_loss} | test_loss: {test_loss} | test_spearman: {accuracy.statistic}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
